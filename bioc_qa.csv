AID,QID,Question,Response
answer1,question1,"I am a bit confused about the concepts of the 3 things: FDR, FDR adjusted p-value and q-value, which I initially thought I was clear about. Are FDR adjusted p-value the same as q-value? (my understanding is that FDR adjusted p-value = original p-value * number of genes/rank of the gene, is that right?) When people say xxx genes are differentially expressed with an FDR cutoff of 0.05, does that mean xxx genes have an FDR adjusted p-value smaller than 0.05?","The thing to understand is that terms like FDR and q-value were defined in specific ways by their original inventors but are used in more generic ways by later researchers who adapt, modify or use the ideas.The term ""false discovery rate (FDR)"" was created by Benjamini and Hochberg in their 1995 paper. They gave a particular definition of what they meant by FDR.  Their procedure accepted or rejected hypotheses, but did not produce adjusted p-values.Benjamini and Yekutieli presented another more conservative algorithm to control the FDR in a 2001 paper. Same definition of FDR, but a different algorithm.In 2002, I re-interpreted the Benjamini and Hochberg (BH) and Benjamini and Yekutieli (BY) procedures in terms of adjusted p-values. I implemented the resulting algorithms in the function p.adjust() in the stats package, and used them in the limma package, and this lead to the concept of an FDR adjusted p-value. The terminology used by the p.adjust() function and limma packages has lead people to refer to ""BH adjusted p-values"".The adjusted p-value definition that you give is essentially the same as the BH adjusted p-value, except that you omitted the last step in the procedure. Your definition as it stands is not an increasing function of the original p-values.In 2002, John Storey created a new definition of ""false discovery rate"". Storey's definition is based on Benjamini and Hochberg's original idea, but is mathematically a bit more flexible. John Storey also created the terminology ""q-value"" for a quantity that estimates his definition of FDR. He implemented q-value estimation procedures in an R package called qvalue. Another important but often overlooked difference is the idea of FDR ""estimation"" vs FDR ""control"". The qvalue package attempts to give a more or less unbiased estimate of the FDR, so the true FDR is about equally likely to be greater or less in practice. The BH approach instead controls the expected FDR. It guarantees that the true FDR rate will be less than the specified rate on average if you do an exactly similar experiment over and over again. So the BH approach is slightly more conservative than qvalue. The BH properties hold regardless of the number of p-values, while qvalue is asymptotic, so the BH approach is more robust than qvalue when the number of hypotheses being tested isn't very large.So, strictly speaking, the q-value and the FDR adjusted p-value are similar but not quite the same. However the terms q-value and FDR adjusted p-value are often used generically by the Bioconductor community to refer to any quantity that controls or estimates any definition of the FDR. In this general sense the terms are synonyms.The lesson to draw from this is that different methods and different packages are trying to do slighty different things and give slightly different results, and you should always cite the specific software and method that you have used."
answer2,question2,"I am working on RNA-Seq data. I'm using DESeq2 for my analysis. I have 20 samples from 3 batches. I am testing for 2 conditions, cond1 and cond2.dds <- DESeqDataSetFromMatrix(countData = countTable3, colData = coldata, design = ~cond1 * cond2). When i performed PCA, I could clearly see some batch effect. I read in the forum that adding batch to the design in DESeq removes the batch effect. But I am not sure if this is the right way to go about it because I can still see the same batch effect dds <-DESeqDataSetFromMatrix(countData = countTable3, colData = coldata, design = ~batch+cond1*cond2) I tried using Combat but I'm unable to use combat results in DESeq. It gives me the following error.Error in DESeqDataSet(se, design = design, ignoreRank) : some values in assay are negative . I am not sure if removeBatchEffects() function can be used with DESeq. Can someone please help me out here.","Just to be clear, there's an important difference between removing a batch effect and modelling a batch effect. Including the batch in your design formula will model the batch effect in the regression step, which means that the raw data are not modified (so the batch effect is not removed), but instead the regression will estimate the size of the batch effect and subtract it out when performing all other tests. In addition, the model's residual degrees of freedom will be reduced appropriately to reflect the fact that some degrees of freedom were ""spent"" modelling the batch effects. This is the preferred approach for any method that is capable of using it (this includes DESeq2). You would only remove the batch effect (e.g. using limma's removeBatchEffect function) if you were going to do some kind of downstream analysis that can't model the batch effects, such as training a classifier.Batch effects are gene-specific, and DESeq2 fits gene-specific coefficients for the batch term. If you want to get an idea how much batch variability contributes to a PCA plot, I've recommended the following approach on the support site before: variance stabilize the counts, apply limma's removeBatchEffect to assay(vsd), then use plotPCA to plot the residuals. An example: Make some simulated data with a batch effect: dds <- makeExampleDESeqDataSet(betaSD 1,interceptMean=10) dds$batch <- factor(rep(c(""A"",""B""),each=6)) VST, remove batch effect, then plotPCA: vsd <- vst(dds) plotPCA(vsd, ""batch"") assay(vsd) <- limma::removeBatchEffect(assay(vsd), vsd$batch) plotPCA(vsd, ""batch"")"
answer3,question3,"I am new in this kind of analysis and I have a .csv file containing RNA-Seq data from different cell lines (with at least 3 replicates) normalised to TPM already, unfortunately I cannot access to the raw counts files.Is there a way I can follow to obtain the p-values, t-values and padj starting from TPM values in order to perform a differential expression analysis? I read about DESeq, DESeq2, EdgeR, limma and it looks like if all the R packages would ask for the raw counts and not from TPM values?","There is no good way to do a DE analysis of RNA-seq data starting from the TPM values. TPMs just throw away too much information about the original count sizes. Sorry, but I'm not willing to make any recommendations, except to dissuade people from thinking that TPMs are an adequate summary of an RNA-seq experiment.Note that it is not possible to create a DGEList object or CPM values from TPMs, so trying to use code designed for these sort of objects will be counter-productive.I see that some people in the literature have done limma analyses of the log(TPM+1) values and, horrible though that is, I can't actually think of anything better, given TPMs and existing software. One could make this a little better by using eBayes with trend=TRUE and by using arrayWeights() to try to partially recover the library sizes."
answer4,question4,"I am testing salmon and kallisto for RNA-seq. Both tools output ESTIMATED counts and TPM.  My questions are: 1. from the help of tximport function:countsFromAbundance:character, either ""no"" (default), ""scaledTPM"", or ""lengthScaledTPM"", for whether to generate estimated counts using abundance estimates scaled up to library size (scaledTPM) or additionally scaled using the average transcript length over samples and the library size (lengthScaledTPM). if using scaledTPM or lengthScaledTPM, then the counts are no longer correlated with average transcript length, and so the length offset matrix should not be used.To my understanding, TPM is a unit that scaled by (effective) feature length first and then sequencing depth. So, what are scaledTPM and lengthScaled TPM? does tximport use the estimate counts to get the TPM? 2. what's the difference among the TPM output by salmon/kallisto and the TPM returned by tximport function? 3. How does tximport mathematically convert counts to TPM if use the estimated counts to get the TPM?","To answer your questions:1) scaledTPM is TPM's scaled up to library size, while lengthScaledTPM first multiplies TPM by feature length and then scales up to library size. These are then quantities that are on the same scale as original counts, except no longer correlated with feature length across samples.2) No difference. tximport is simply importing the TPMs and providing them back to the user as a matrix (txOut=TRUE), or summarizing these values among isoforms of a gene (txOut=FALSE).3) Counts are never converted to TPMs. The default is to import the estimated counts and estimated TPMs from the quantification files, and then summarize these to the gene level."
answer5,question5,"In all RNA-seq analysis applications they talk about the dispersion of a gene. As far as I understood, it is not a variance of the normalized counts for a given gene. It is somehow much more complicated. But what would a dispersion of 0.19 or a dispersion of 0.80 tell me? Can I still interpret it as a variance of a gene?","The most complete explanation of what the dispersion means from a scientific point of view is probably in the edgeR glm paper: http://nar.oxfordjournals.org/content/40/10/4288. See the first section of Results in conjunction with the first section of Methods. That article characterized sqrt(dispersion) as the ""biological coefficient of variation (BCV)"", and that is the terminology we have used since in the edgeR articles and documentation. The BCV is the relative variability of expression between biological replicates. If you estimate dispersion = 0.19, then sqrt(dispersion) = BCV = 0.44. This means that the expression values vary up and down by 44% between replicates.An important point, that is easy to miss, is that the BCV measures the relative variability of true expression levels, not the variability of measured expression levels. The BCV represents the relative variability that you would observe if you were able to measure the true expression levels perfectly in each RNA sample, even though one can't actually do that. It represents the variability that remains after the Poisson variability from sequencing has been removed.To repeat, BCV does not represent the variability between observed expression levels. It is the variability of true expression levels. You cannot measure BCV using an undergraduate formula from the observed counts or RPKM values."
answer6,question6,"I know findOverlaps() from GenomicRanges package does pretty good job for finding overlapped regions.I have studied whole vignette of GenomicRanges, BiocParallel pakages. findOverlaps function is very well done, but I need element wise operation across several GRanges objects simultaneously. I come up following reproducible example and put my desire output as well.Objective: find out overlapped regions across multiple GRanges objects simultanously (a.k.a, element-wise), aim to provide co-localization test and to save discarded enriched regions that was discarded in single trail, but discarded enriched regions will be saved by combining evidence of multiple Chip-seq experiment sample (find out its overlapped regions across across multiple GRanges objects in parallel). for example:Step 1: setuplibrary(GenomicRanges)a <- GRanges(  seqnames=Rle(c(""chr1"", ""chr2"", ""chr3"", ""chr4""), c(3, 2, 1, 2)),  ranges=IRanges(seq(1, by=5, len=8), seq(4, by=5, len=8)),  rangeName=letters[seq(1:8)],  score=sample(1:20, 8, replace = FALSE))b <- GRanges(  seqnames=Rle(c(""chr1"", ""chr2"", ""chr3"",""chr4""), c(4, 1, 2, 1)),  ranges=IRanges(seq(2, by=6, len=8), seq(5, by=6, len=8)),  rangeName=letters[seq(1:8)],  score=sample(1:20, 8, replace = FALSE))c <- GRanges(  seqnames=Rle(c(""chr1"", ""chr2"", ""chr3"",""chr4""), c(2, 4, 1,1)),   ranges=IRanges(seq(4, by=4, len=8), seq(7, by=4, len=8)),  rangeName=letters[seq(1:8)],  score=sample(1:15, 8, replace = FALSE))Step 2: I want to do overlap operation simultaneously such as:# each iteration, only take one ranges out of multiple ranges in# querySample (a.k.a, first GRanges objects)queryRange <- a[1]ov_b <- b[subjectHits(findOverlaps(a[1], b))]ov_c <- c[subjectHits(findOverlaps(a[1], c))]Step 3: then, I want to keep only one overlapped ranges from ov_b, ov_c (if there are multiple intersection were found), such as:ov_b_keepOne <- ov_b[which.max(ov_b$score)]  # if multiple overlapped                                             # found, only keep most                                             # significant one.ov_c_KeepOne <- ov_c[which.max(ov_c$score)]Step 4: then, use chisq.test to find out combined pvalue, such as:comb.pval <- chisq.test(c(queryRange$score, ov_b$score, ov_c$score))$p.valueStep 5: then, given threshold value gamma=1e-8,  (for example, comb.pvalue > gamma), then, execute below:res <- GRangesList('querySample'=queryRange,                   'targetSample_1'=ov_b_keepOne,                   'targetSample_2'=ov_c_KeepOne)Step 6: finally, this is my expected results :res.df <- as.data.frame(res)write.table(res.df, file=""Confirmed.bed"")Objective: only take one ranges (a.k.a, each element of GRanges objects) out of multiple ranges, to find its overlapped ranges from multiple GRanges objects simultaneously (seems bplapply function can does this, but I haven't familiar with batch processing in R). I have bottleneck with this problem for a while, still not solved efficiently, any idea or possible approach to save my effort to efficiently develop my packages. Please point me out what should I do?","From the discussion below, an efficient starting point for this work flow seems to be## helper function: identify the overlapping range satisfying FUNkeepone <- function(gr, hitlist, FUN=which.max) {    idx0 <- as(FUN(extractList(gr$score, hitlist)), ""List"")    idx1 <- unlist(extractList(seq_along(gr), hitlist)[idx0])    ## FIXME: what about NA's when there are no matching ranges?    gr[idx1]}ab_hitlist <- as(findOverlaps(a, b), ""List"")ac_hitlist <- as(findOverlaps(a, c), ""List"")test <- (lengths(ab_hitlist) > 0) + (lengths(ac_hitlist) > 0)keep <- test >= minNumOlp_Requireda = a[keep]ab_hitlist = ab_hitlist[keep]ac_hitlist = ac_hitlist[keep]b_one <- keepone(b, ab_hitlist)c_one <- keepone(c, ac_hitlist)comb.pval <-    mapply(function(a, b, c) chisq.test(c(a, b, c))$p.value,           a$score, b_one$score, c_one$score)Updating each stepThe place I want to start is with step 2, which I will replace withab_hitlist <- as(findOverlaps(a, b), ""List"")ac_hitlist <- as(findOverlaps(a, c), ""List"")for the reasons below.I will replace step 4 withcomb.pval  <- mapply(function(a, b, c) {    chisq.test(c(a, b, c))$p.value}, a$score, max(extractList(b$score, ab_hitlist)), max(extractList(c$score, ac_hitlist)))Step 3 is to identify the overlapping range with maximum score. I did this as follows:keepone <- function(gr, hitlist) {    idx0 <- as(which.max(extractList(gr$score, hitlist)), ""List"")    idx1 <- unlist(extractList(seq_along(gr), hitlist)[idx0])    ## FIXME: what about NA's when there are no matching ranges?    gr[idx1]}b_one <- keepone(b, ab_hitlist)c_one <- keepone(c, ac_hitlist)But now that the specification for step 4 has been clarified as the maximum score of the overlapping ranges, and we have the overlapping range, step 4 iscomb.pval  <- mapply(function(a, b, c) chisq.test(c(a, b, c))$p.value,                     a$score, b_one$score, c_one$score) Justification for updated step 2:It is very inefficient to calculate overlaps between 1 range and many ranges. So 'invert' the problem and find all overlapsab_hit <- findOverlaps(a, b)ac_hit <- findOverlaps(a, c)For convenience, split the hits into a list, where each element of the list corresponds to the overlaps for a different queryab_hitlist <- as(ab_hit, ""List"")For your example, this is> ab_hitlistIntegerList of length 8[[1]] 1 2 3 4[[2]] 1 2 3 4[[3]] 1 2 3 4[[4]] 5[[5]] 5[[6]] 6 7[[7]] 8[[8]] 8Convince yourself that ab_hitlist[[1]]> ab_hitlist[[1]][1] 1 2 3 4is the same as subjectHits(findOverlaps(a[1], b)), and so on for a[2], a[3], a[4]. The reason for this change is that taking ranges one-at-a-time scales linearly> library(microbenchmark)> f <- function(a, b)+     lapply(seq_along(a), function(i) subjectHits(findOverlaps(a[i], b)))> g <- function(a, b)+     as(findOverlaps(a, b), ""List"")> a0 <- a[1:4]; a2 <- c(a, a); a4 <- c(a2, a2); a8 <- c(a4, a4); a16 <- c(a8, a8)> microbenchmark(f(a0, b), f(a, b), f(a2, b), times=10)Unit: milliseconds     expr       min        lq      mean    median        uq       max neval cld f(a0, b)  75.21582  76.07495  77.63697  77.76686  78.44264  80.56188    10 a    f(a, b) 152.56619 154.25827 157.34376 156.14110 158.94034 169.31483    10  b  f(a2, b) 309.08562 309.42790 313.71373 312.01247 316.88869 323.32852    10   c> microbenchmark(g(a0, b), g(a, b), g(a2, b), g(a4, b), g(a8, b), g(a16, b),+                times=10)Unit: milliseconds      expr      min       lq     mean   median       uq       max neval cld  g(a0, b) 17.24473 17.32625 17.93150 17.67085 18.01294  20.22646    10   a   g(a, b) 17.33038 17.50243 18.09587 17.73340 18.19032  20.01569    10   a  g(a2, b) 20.46892 20.52388 21.30909 21.11872 21.66259  23.64361    10   a  g(a4, b) 20.51881 20.68018 21.32741 21.17813 21.65589  23.31553    10   a  g(a8, b) 20.55628 20.64502 21.14069 20.86133 21.33573  22.41963    10   a g(a16, b) 20.50725 20.82241 34.97887 21.24104 21.45764 158.24840    10   awhereas the same is not true for creating a hits object (the scaling is not evident from the small example data).Justification for updated step 4.The extractList() function will take a vector-like object as a first argument, and reshape it according to the indexes of the second argument. It is relatively amazingly useful to see that> extractList(b$score, ab_hitlist)IntegerList of length 8[[1]] 12 19 11 6[[2]] 12 19 11 6[[3]] 12 19 11 6[[4]] 7[[5]] 7[[6]] 10 20[[7]] 14[[8]] 14extracts the scores and reshapes them into the correct geometry of the hits.The calculation of p-values applies chisq.test() to the element-wise 'parallel' vectors a$score, and from the discussion in the comments below to max(extractList(b$score, ab_hitlist)), and max(extractList(c$score, ac_hitlist)). Socomb.pval <- mapply(function(a, b, c) {    chisq.test(c(a, b, c))$p.value}, a$score, max(extractList(b$score, ab_hitlist)), max(extractList(c$score, ac_hitlist)))The result is> comb.pval [1] 1.380288e-04 1.290620e-04 3.281973e-05 5.494160e-01 4.821936e-01 [6] 1.813352e-01 3.805041e-03 8.590223e-02All other operations in can be 'vectorized' to be efficient, but chisq.test() cannot be (I bet there's a function that does do this in a vectorized way?). It is therefore a candidate for parallel evaluation, but I would only do this at the end, when it becomes apparent that this is a rate-limiting step.library(BiocParallel)comb.pval <- bpmapply(function(a, b, c) {    chisq.test(c(a, b, c))$p.value}, a$score, max(extractList(b$score, ab_hitlist)), max(extractList(c$score, ac_hitlist)))Justification for updated step 3.which.max(extractList(b$score, ab_hitlist))returns a plain vector of indexes in to each of the list elements of the hitlist. Coercing this to a Listidx0 <- as(which.max(extractList(gr$score, hitlist)), ""List"")restores the 'geometry' of the vector, so that we have a list of length(ab_hitline). We then create an index into the grang that we're interested in, cast it in the appropriate geometry, and subset with the indexidx1 <- unlist(extractList(seq_along(b), ab_hitlist)[idx0])These are the indexes into the original ranges, so we can retrieve them withb[idx1]A little complicated, so we implement this as a small helper functionkeepone <- function(gr, hitlist) {    idx0 <- as(which.max(extractList(gr$score, hitlist)), ""List"")    idx1 <- unlist(extractList(seq_along(gr), hitlist)[idx0])    gr[idx1]}"
answer7,question7,"I have just downloaded CNV level 3 files from TCGA database. In these files there are three columns: chromosome, start, and end which presents the coordinates of genes. Now, I would like to map them to gene symbols, but I don't know. Can you tell me how to do that?","I wrote two helper functions, explained belowgeneRanges <-     function(db, column=""ENTREZID""){    g <- genes(db, columns=column)    col <- mcols(g)[[column]]    genes <- granges(g)[rep(seq_along(g), elementLengths(col))]    mcols(genes)[[column]] <- as.character(unlist(col))    genes}splitColumnByOverlap <-    function(query, subject, column=""ENTREZID"", ...){    olaps <- findOverlaps(query, subject, ...)    f1 <- factor(subjectHits(olaps),                 levels=seq_len(subjectLength(olaps)))    splitAsList(mcols(query)[[column]][queryHits(olaps)], f1)}Load the GenomicRanges packagelibrary(GenomicRanges)Get the CNV regions into a GRanges instance. For instance, if the data is in a BED file, perhaps using the rtracklayer packagecnv = rtracklayer::import(""my.bed"")or if in a tab-delimited filedf = read.delim(""my.tsv"")cnv = makeGRangesFromDataFrame(df)(if you're a fan of dplyr / magrittr style pipes then cnv = read.delim(""my.tsv"") %>% makeGRangesFromDataFrame() works, too). Here I make a couple of genomic ranges 'by hand'> cnv = GRanges(c(""chr1"", ""chr2""), IRanges(94312388,244006886))> cnvGRanges object with 2 ranges and 0 metadata columns:      seqnames                ranges strand         <Rle>             <IRanges>  <Rle>  [1]     chr1 [94312388, 244006886]      *  [2]     chr2 [94312388, 244006886]      *  -------  seqinfo: 2 sequences from an unspecified genome; no seqlengthsLoad the Homo.sapiens packagelibrary(Homo.sapiens)Find the genomic coordinates for each (entrez) gene, as well as the gene symbol> gns = geneRanges(Homo.sapiens, column=""SYMBOL"")> gnsGRanges object with 23056 ranges and 1 metadata column:        seqnames                 ranges strand   |      SYMBOL           <Rle>              <IRanges>  <Rle>   | <character>      1    chr19 [ 58858172,  58874214]      -   |        A1BG     10     chr8 [ 18248755,  18258723]      +   |        NAT2    100    chr20 [ 43248163,  43280376]      -   |         ADA   1000    chr18 [ 25530930,  25757445]      -   |        CDH2  10000     chr1 [243651535, 244006886]      -   |        AKT3    ...      ...                    ...    ... ...         ...   9991     chr9 [114979995, 115095944]      -   |       PTBP3   9992    chr21 [ 35736323,  35743440]      +   |       KCNE2   9993    chr22 [ 19023795,  19109967]      -   |       DGCR2   9994     chr6 [ 90539619,  90584155]      +   |    CASP8AP2   9997    chr22 [ 50961997,  50964905]      -   |        SCO2  -------  seqinfo: 93 sequences (1 circular) from hg19 genomeFind which genes overlap which copy number regions> symInCnv = splitByOverlap(gns, cnv, ""SYMBOL"")> symInCnvCharacterList of length 2[[""1""]] AKT3 MIR942 MIR190B MIR760 MIR921 ... FAM20B LPGAT1 KIF14 RBM8A NR1I3[[""2""]] TANK SNORD11B MIR933 LOC100129175 CCNT2-AS1 ... ZEB2 FARP2 TLK1 CD302The result is a CharacterList (list of character vectors) where each element contains the genes overlapping the corresponding CNV region.A CharacterList is very convenient to work with, but use as.vector(symInCnv) to work as a plain list-of-characters, or unstrsplit(symInCnv, sep="", "") to paste the symbols in each CNV together.Here's the first helper function, disectedgeneRanges <-     function(db, column=""ENTREZID"")'db' is a so-called OrganismDb, containing information about genes (e.g., ENTREZ or SYMBOL gene ids) as well as gene models (e.g., TXSTART, TXEND). The Homo.sapiens OrangismDb is based on the org.Hs.eg.db gene annotation package, and the TxDb.Hsapiens.UCSC.hg19.knownGene package; it is relatively easy to make a custom package if you have different annotations.'column' is the type of gene identifier you are interested in mapping to. See columns(Homo.sapiens) for some ideas.{    g <- genes(db, columns=column)This line extracts the coordinates (min and max) of each gene, as well as the column (e.g., SYMBOL associated with that gene.The TxDb is organized around a primary key (the ENTREZID), and there may be several gene SYMBOLS per ENTREZID. The next few lines makes each SYMBOL map to a single genomic location.    col <- mcols(g)[[column]]    genes <- granges(g)[rep(seq_along(g), elementLengths(col))]    mcols(genes)[[column]] <- as.character(unlist(col))    genes}The end result is a GRanges instance, with a metadata column corresponding to the type of gene identifier of interest.The basic idea behind splitByOverlap is that we can find which gene coordinates overlap which copy number variant coordinates, and then split the column of gene identifiers into lists corresponding to the regions of overlapsplitByOverlap <-    function(x, f, column=""ENTREZID"", ...)'query' is the gene coordinates, 'subject' the copy number coordinates. 'column' needs to match 'column' in the geneRanges() function.{    olaps <- findOverlaps(query, subject, ...)findOverlaps() is a very powerful function. It returns a 'Hits' object that has two parallel vectors. The vectors can be extracted with queryHits() and subjectHits(). queryHits() are the indexes of the queries (the gene coordinates) that overlap the corresponding subjectHits(), i.e., the indexes of the subjects, the copy number regions. The next lines line up the query column identifier (e.g., gene SYMBOL) that overlaps each subject.    f1 <- factor(subjectHits(olaps), levels=seq_len(subjectLength(olaps)))    splitAsList(mcols(x)[[column]][queryHits(olaps)], f1)the use of factor() with exactly as many levels as there are subjects ensures that the splitAsList() command returns a 1:1 mapping between the subjects (copy number regions) and the genes in the corresponding CharacterList.Another approach uses Steffen Durink's fine biomaRt package, a  contribution to Bioconductor of many year's standing.  In its simplest form just two lines of code gets you all the genes in one copy number region:library(biomaRt)mart <- useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"")results <- getBM(attributes = c(""hgnc_symbol"", ""chromosome_name"",                                  ""start_position"", ""end_position""),                 filters = c(""chromosome_name"", ""start"", ""end""),                  values=list(1, 94312388, 96000000),                 mart=mart)dim(results)  # 34 hits, only 12 with gene symbolsIf you are reading your data from a file (the TCGA level 3 data you mentioned), and if you wish to filter the regions, this approach may be useful:library(GenomicRanges)filename <- ""TRIBE_p_TCGAaffx_B1_2_GBM_Nsp_GenomeWideSNP_6_A01_155716.nocnv_hg19.seg.txt""tbl <- read.table(filename, sep=""\t"", as.is=TRUE, header=TRUE);gr <- makeGRangesFromDataFrame(tbl)With the data in a standard Bioconductor GRanges object, filtering the data becomes easy.  Let's submit just very short copy number regions to biomaRt:gr.short <- subset(gr, width < 100)length(gr) # 117 regionslength(gr.short) # just 2 regionsgr.shortGRanges object with 2 ranges and 0 metadata columns:      seqnames                 ranges strand         <Rle>              <IRanges>  <Rle>  [1]        2 [109993208, 109993264]      *  [2]       10 [114452689, 114452764]      * We need a different biomaRt filter in order to retrieve gene symbols from multiple regions with one call, and we have to specify the copy number the regions in a different format:regions <- paste(seqnames(gr.short), start(gr.short), end(gr.short), sep="":"")regions[1] ""2:109993208:109993264""  ""10:114452689:114452764""results <- getBM(attributes = c(""hgnc_symbol"", ""chromosome_name"",                                 ""start_position"",""end_position""),                 filters = c(""chromosomal_region""),                 values=regions,                 mart=mart)biomaRt returns two genes, one each from the two short regions:hgnc_symbol chromosome_name start_position end_position 1      ABLIM1              10      114431113    114685003 2   LINC01123               2      109987063    109996140You may wish to create a GRanges object from this data.frame in order to simplify subsequent analyses.  In general we recommend this: the standard Bioc data types promote interoperability between different packages.  In this particular case -- gene symbols mapped to copy number regions -- you may wish to preserve the original TCGA-reported regions in a GRanges object, and then represent the mapped genes as GRanges metadata.  However (and as Herve' helpfully pointed out to me) this relationship could be one-to-many, or many-to-one, and the data structure can get a little complicated - though still very useful."
answer8,question8,"How can I filter out the genes with low read counts using DESeq2? Is this the right approach:dds<-DESEq(dt)count<-counts(dds,normalize=TRUE)filter<-rowsum(count)> 10","If you want to filter out genes with low expression, you can do so before running DESeq:dds <- estimateSizeFactors(dds)idx <- rowSums( counts(dds, normalized=TRUE) >= 5 ) >= 3This would say, e.g. filter out genes where there are less than 3 samples with normalized counts greater than or equal to 5.then:dds <- dds[idx,]dds <- DESeq(dds)However, you typically don't need to pre-filter because independent filtering occurs within results() to save you from multiple test correction on genes with no power (see ?results and the vignette section about independent filtering, or the paper). The main reason to pre-filter would be to increase speed. Designs with many samples and many interaction terms can slow down on genes which have very few reads."
answer9,question9,"I am analysing my RNA-Seq data with DESeq2. At the end I would like to convert significantly expressed  ensembl IDs to GeneSymbols. I am using AnnotationDbi for this. However, I realized that not all the ensembl IDs are converted to Gene Symbols. 25072 out of 48607 returned as NA. More than 11000 of these IDs are actually significantly differentially expressed .  So to double check, I put the IDs which got ""NA"" for Gene Symbol to Biomart and it converted them to Gene symbols. So now I am confused, am I doing something wrong ? Or is there any other alternative to extract all Gene symbols?","You can use the ensembldb package to do the mapping between Ensembl gene IDs and gene names (or symbols). You would need also one of the EnsDb packages providing the actual annotation (such as EnsDb.Hsapiens.v75 for genome release GRCh37 or EnsDb.Hsapiens.v79 vor GRCh38). Check the ensembldb vignette for more information (http://www.bioconductor.org/packages/release/bioc/vignettes/ensembldb/inst/doc/ensembldb.html).You could basically use the same AnnotationDbi call that you use, but provide the EnsDB object instead of the org.Hs.eg.db object.Just one clarification: the gene names that are listed above in your table are not gene symbols. These are rather the names for the gene that are provided by Ensembl. For protein coding genes the gene names correspond however to the HGNC symbols.The OrgDb packages are a collection of data from many different sources, NCBI, UCSC, Ensembl, etc. The packages are Entrez gene centric in that we start with the list of Entrez gene ids from NCBI and annotate to that id. Data downloaded from Ensembl is matched to the Entrez gene id, if no mapping between the two exists then the Ensembl id doesn't end up in the OrgDb package.Starting from three example ENSEMBL gene IDs:ensemblGenes <- c(""ENSG00000108958"", ""ENSG00000123009"", ""ENSG00000124399"") symbols <- c(""AC016292.3"", ""NME2P1"", ""NDUFB4P12"")As you said, the OrgDb package doesn't have data for these Ensembl ids:> select(org.Hs.eg.db, + key=ensemblGenes, columns=columns(org.Hs.eg.db), + keytype=""ENSEMBL"") Error in .testForValidKeys(x, keys, keytype, fks) : None of the keys entered are valid keys for 'ENSEMBL'. Please use the keys method to see a listing of valid arguments.Using the symbols instead, we see the OrgDb has data for some of these genes but no Ensembl -> Entrez id mapping:> select(org.Hs.eg.db, key=symbols, + columns=c(""ENTREZID"", ""ENSEMBL""), + keytype=""SYMBOL"") 'select()' returned 1:1 mapping between keys and columns SYMBOL ENTREZID ENSEMBL 1 AC016292.3 <NA> <NA> 2 NME2P1 283458 <NA> 3 NDUFB4P12 402175 <NA>biomaRt also shows no mapping between the two and is missing the symbol for the first gene:library(biomaRt) mart<- useDataset(""hsapiens_gene_ensembl"", useMart(""ENSEMBL_MART_ENSEMBL"")) >getBM(filters=""ensembl_gene_id"", + attributes=c(""ensembl_gene_id"", ""entrezgene"", + ""hgnc_symbol""), + values=ensemblGenes, + mart=mart) ensembl_gene_id entrezgene hgnc_symbol 1 ENSG00000108958 NA 2 ENSG00000123009 NA NME2P1 3 ENSG00000124399 NA NDUFB4P12  Using Jo's EnsDb.Hsapiens.v79, it looks like the Ensembl id is called GENEID so we use that as 'keytype'. It also confirms no mapping between Ensembl and Entrez but it does return a value for all the symbols.> select(EnsDb.Hsapiens.v79, key=ensemblGenes, + columns=c(""ENTREZID"", ""SYMBOL""), + keytype=""GENEID"") GENEID ENTREZID SYMBOL 1 ENSG00000108958 AC016292.3 2 ENSG00000123009 NME2P1 3 ENSG00000124399 RP11-663P9.2So for the task of mapping Ensembl ids to gene symbols it looks like the ensembl package is the most comprehensive. "
answer10,question10,"How do I merge a list of GRanges? What I want is a GRanges object containing the union of all the ranges and the metadata in the list. I could loop over the list, taking the union of each pair of GRanges, but the union loses the metadata. Is there a way to merge all my GRanges into one giant GRanges, with metadata? What I ultimately want is a data.frame of counts so that I can input it into limma.","Merge is a pretty vague term. My understanding is that you want to concatenate all the GRanges objects in the list. In the R world, this concatenation is performed with c().One complication here is that the arguments to your call to c() are in a list. So you need to use do.call(""c"", list_of_arguments):l1 <- list(1:5, 21:22, 31:34)do.call(""c"", l1)  # same as c(l1[[1]], l1[[2]], l1[[3]])# [1]  1  2  3  4  5 21 22 31 32 33 34So with your list of GRanges objects do.call(""c"", l3) should do what you want.Alternatively, you can dounlist(as(l, ""GRangesList""))which converts the base list object l to a GRangesList and then uses the GRL-specific unlist to merge to single GRanges object."
