{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Azure GPT-4 based model with RAG store of Top 500 Bioconductor package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relevant keys from Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai, requests\n",
    "    \n",
    "## Declare keys\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "search_key = os.getenv(\"OPENAI_SEARCH_KEY\")\n",
    "endpoint = os.getenv(\"OPENAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the deployment name, endpoint and the name of the search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai.api_type = \"azure\"\n",
    "# Azure OpenAI on your own data is only supported by the 2023-08-01-preview API version\n",
    "openai.api_version = \"2023-08-01-preview\"\n",
    "\n",
    "# Azure OpenAI setup\n",
    "openai.api_base = endpoint\n",
    "openai.api_key = api_key \n",
    "deployment_id = \"gpt-4-test\" \n",
    "\n",
    "# Azure AI Search setup\n",
    "search_endpoint = \"https://hmsaisearch.search.windows.net\"; # Add your Azure AI Search endpoint here\n",
    "search_index_name = \"bioc-top-500\"; # Add your Azure AI Search index name here\n",
    "    \n",
    "# https://hms-it-openai.openai.azure.com/openai/deployments/gpt-4-test/extensions/chat/completions?api-version=2024-02-15-preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"client\" object with AzureOpenAI with the correct api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hms-it-openai.openai.azure.com/openai/deployments/gpt-4-test/extensions/chat/completions?api-version=2024-02-15-preview\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    base_url=f\"{endpoint}/openai/deployments/{deployment_id}/extensions\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-08-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This step is to set up the RAG store i.e \"bring your own data\" search index that has to be registered with the deployment in hand - `gpt-4-test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_byod(deployment_id: str) -> None:\n",
    "    \"\"\"Sets up the OpenAI Python SDK to use your own data for the chat endpoint.\n",
    " \n",
    "    :param deployment_id: The deployment ID for the model to use with your own data.\n",
    "\n",
    "    To remove this configuration, simply set openai.requestssession to None.\n",
    "    \"\"\"\n",
    "\n",
    "    class BringYourOwnDataAdapter(requests.adapters.HTTPAdapter):\n",
    "\n",
    "     def send(self, request, **kwargs):\n",
    "         request.url = f\"{openai.api_base}/openai/deployments/{deployment_id}/extensions/chat/completions?api-version={openai.api_version}\"\n",
    "         return super().send(request, **kwargs)\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Mount a custom adapter which will use the extensions endpoint for any call using the given `deployment_id`\n",
    "    session.mount(\n",
    "        prefix=f\"{openai.api_base}/openai/deployments/{deployment_id}\",\n",
    "        adapter=BringYourOwnDataAdapter()\n",
    "    )\n",
    "\n",
    "    openai.requestssession = session\n",
    "\n",
    "setup_byod(deployment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic function to ask a question from our BYOD model with GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        model=deployment_id,\n",
    "        extra_body={\n",
    "            \"dataSources\": [\n",
    "                {\n",
    "                    \"type\": \"AzureCognitiveSearch\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": search_endpoint,\n",
    "                        \"key\": search_key,\n",
    "                        \"indexName\": search_index_name,\n",
    "                        \"roleInformation\": \"Act as an expert in the R programming language and the Bioconductor suite of packages.  ​\\n\\nYour job is to advise users on the usage of the various Bioconductor packages considering the documents you have in the data store.  ​\\n\\nTo complete this task, you can use the data you have stored that contain the vignettes of all the packages in Bioconductor and all the reference files of every function in every package of Bioconductor. ​You may also answer some general R, general programming, or Biomedical information.\\n\\nIf you do not know the answer ask the user to refer to https://bioconductor.org. \\n\\nAdd a disclaimer at the end of each response saying this model works only on the top 500 most used Bioconductor packages.\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"{completion.choices[0].message.role}: {completion.choices[0].message.content}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `context` is in the model_extra for Azure\n",
    "# print(f\"\\nContext: {completion.choices[0].message.model_extra['context']['messages'][0]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for SummarizedExperiemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarized experiment \n",
    "question = \"How many classes are there in the Summarized Experiment package? Just give me a number.\"\n",
    "\n",
    "ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for DESeq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: DESeq2 estimates one size factor for each sample in your experiment. Therefore, if your experiment has 5 samples, DESeq2 will estimate 5 size factors [doc1][doc2].\n",
      "\n",
      "Please note that this information is based on the top 500 most used Bioconductor packages. For more specific or detailed information, please refer to the official Bioconductor documentation at https://bioconductor.org.\n"
     ]
    }
   ],
   "source": [
    "## DESeq2\n",
    "\n",
    "question = \"DESeq2 performs normalization by estimating size factors for each sample. If your experiment has 5 samples, how many size factors will DESeq2 estimate?\"\n",
    "\n",
    "ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for limma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: In the context of a case-control study with 30 control samples and 30 case samples, when you fit a linear model using the limma package in R, you will estimate two coefficients. \n",
      "\n",
      "The first coefficient is the intercept, which represents the baseline level of gene expression in the reference group (usually the control group). The second coefficient is the slope, which represents the difference in gene expression between the case group and the control group. \n",
      "\n",
      "This is under the assumption that no additional covariates are included in the model. If additional covariates were included, there would be an additional coefficient estimated for each covariate [doc3].\n",
      "\n",
      "Please note that this model works only on the top 500 most used Bioconductor packages. For more specific information, please refer to https://bioconductor.org.\n"
     ]
    }
   ],
   "source": [
    "## Limma\n",
    "question = \"You use limma to analyze RNA-seq data from a case-control study with 30 control samples and 30 case samples. After fitting the linear model, how many coefficients will be estimated by limma for the gene expression data (assuming no additional covariates are included in the model)?\"\n",
    "\n",
    "ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for SingleCellExperiment\n",
    "\n",
    "This questions doesn't give a good answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The SingleCellExperiment object, which is extended by the TreeSummarizedExperiment class, does not have a specific number of slots mentioned in the provided documents. However, the TreeSummarizedExperiment class, which extends the SingleCellExperiment class, has five additional slots: rowTree, rowLinks, colTree, colLinks, and referenceSeq [doc1][doc2]. \n",
      "\n",
      "For more detailed information about the SingleCellExperiment object, you may want to refer to the SingleCellExperiment package documentation on the Bioconductor website (https://bioconductor.org).\n",
      "\n",
      "Please note that this model works only on the top 500 most used Bioconductor packages.\n"
     ]
    }
   ],
   "source": [
    "question = \"how many slots does the SingleCellExperiment object have?\"\n",
    "\n",
    "ask_question(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
