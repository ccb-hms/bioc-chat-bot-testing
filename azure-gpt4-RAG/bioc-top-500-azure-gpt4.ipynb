{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Azure GPT-4 based model with RAG store of Top 500 Bioconductor package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relevant keys from Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai, requests\n",
    "    \n",
    "## Declare keys\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "search_key = os.getenv(\"OPENAI_SEARCH_KEY\")\n",
    "endpoint = os.getenv(\"OPENAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the deployment name, endpoint and the name of the search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai.api_type = \"azure\"\n",
    "# Azure OpenAI on your own data is only supported by the 2023-08-01-preview API version\n",
    "openai.api_version = \"2023-08-01-preview\"\n",
    "\n",
    "# Azure OpenAI setup\n",
    "openai.api_base = endpoint\n",
    "openai.api_key = api_key \n",
    "deployment_id = \"gpt-4-test\" \n",
    "\n",
    "# Azure AI Search setup\n",
    "search_endpoint = \"https://hmsaisearch.search.windows.net\"; # Add your Azure AI Search endpoint here\n",
    "search_index_name = \"bioc-top-500\"; # Add your Azure AI Search index name here\n",
    "    \n",
    "# https://hms-it-openai.openai.azure.com/openai/deployments/gpt-4-test/extensions/chat/completions?api-version=2024-02-15-preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"client\" object with AzureOpenAI with the correct api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hms-it-openai.openai.azure.com/openai/deployments/gpt-4-test/extensions/chat/completions?api-version=2024-02-15-preview\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    base_url=f\"{endpoint}/openai/deployments/{deployment_id}/extensions\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-08-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This step is to set up the RAG store i.e \"bring your own data\" search index that has to be registered with the deployment in hand - `gpt-4-test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_byod(deployment_id: str) -> None:\n",
    "    \"\"\"Sets up the OpenAI Python SDK to use your own data for the chat endpoint.\n",
    " \n",
    "    :param deployment_id: The deployment ID for the model to use with your own data.\n",
    "\n",
    "    To remove this configuration, simply set openai.requestssession to None.\n",
    "    \"\"\"\n",
    "\n",
    "    class BringYourOwnDataAdapter(requests.adapters.HTTPAdapter):\n",
    "\n",
    "     def send(self, request, **kwargs):\n",
    "         request.url = f\"{openai.api_base}/openai/deployments/{deployment_id}/extensions/chat/completions?api-version={openai.api_version}\"\n",
    "         return super().send(request, **kwargs)\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Mount a custom adapter which will use the extensions endpoint for any call using the given `deployment_id`\n",
    "    session.mount(\n",
    "        prefix=f\"{openai.api_base}/openai/deployments/{deployment_id}\",\n",
    "        adapter=BringYourOwnDataAdapter()\n",
    "    )\n",
    "\n",
    "    openai.requestssession = session\n",
    "\n",
    "setup_byod(deployment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic function to ask a question from our BYOD model with GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        model=deployment_id,\n",
    "        extra_body={\n",
    "            \"dataSources\": [\n",
    "                {\n",
    "                    \"type\": \"AzureCognitiveSearch\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": search_endpoint,\n",
    "                        \"key\": search_key,\n",
    "                        \"indexName\": search_index_name,\n",
    "                        \"roleInformation\": \"Act as an expert in the R programming language and the Bioconductor suite of packages.  ​\\n\\nYour job is to advise users on the usage of the various Bioconductor packages considering the documents you have in the data store.  ​\\n\\nTo complete this task, you can use the data you have stored that contain the vignettes of all the packages in Bioconductor and all the reference files of every function in every package of Bioconductor. ​You may also answer some general R, general programming, or Biomedical information.\\n\\nIf you do not know the answer ask the user to refer to https://bioconductor.org. \\n\\nAdd a disclaimer at the end of each response saying this model works only on the top 500 most used Bioconductor packages.\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"{completion.choices[0].message.role}: {completion.choices[0].message.content}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `context` is in the model_extra for Azure\n",
    "# print(f\"\\nContext: {completion.choices[0].message.model_extra['context']['messages'][0]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for SummarizedExperiemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The SummarizedExperiment package contains two classes: SummarizedExperiment and RangedSummarizedExperiment [doc1].\n"
     ]
    }
   ],
   "source": [
    "## summarized experiment \n",
    "question = \"How many classes are there in the Summarized Experiment package? Just give me a number.\"\n",
    "\n",
    "ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for DESeq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: DESeq2 estimates one size factor for each sample in your experiment. Therefore, if your experiment has 5 samples, DESeq2 will estimate 5 size factors [doc1]. \n",
      "\n",
      "Please note that this model works only on the top 500 most used Bioconductor packages.\n"
     ]
    }
   ],
   "source": [
    "## DESeq2\n",
    "\n",
    "question = \"DESeq2 performs normalization by estimating size factors for each sample. If your experiment has 5 samples, how many size factors will DESeq2 estimate?\"\n",
    "\n",
    "ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for limma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: In a case-control study design with no additional covariates, the linear model in limma will estimate two coefficients for each gene: one for the intercept and one for the group effect (case vs control). The intercept represents the baseline gene expression level (usually corresponding to the control group), and the group effect represents the difference in gene expression between the case and control groups [doc2][doc4].\n",
      "\n",
      "Please note that this model works only on the top 500 most used Bioconductor packages. For more complex models or less common packages, please refer to the specific package documentation or https://bioconductor.org.\n"
     ]
    }
   ],
   "source": [
    "## Limma\n",
    "question = \"You use limma to analyze RNA-seq data from a case-control study with 30 control samples and 30 case samples. After fitting the linear model, how many coefficients will be estimated by limma for the gene expression data (assuming no additional covariates are included in the model)?\"\n",
    "\n",
    "ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for SingleCellExperiment\n",
    "\n",
    "This questions doesn't give a good answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The SingleCellExperiment object is a complex object with multiple slots. However, the exact number of slots is not specified in the retrieved documents. The SingleCellExperiment object is designed to store single-cell experiment data, including assay data, feature data, and cell metadata. It is a part of the Bioconductor project and is used extensively in R packages for single-cell data analysis.\n",
      "\n",
      "For detailed information about the structure and slots of the SingleCellExperiment object, you can refer to the official Bioconductor documentation at https://bioconductor.org.\n",
      "\n",
      "Please note that this model works only on the top 500 most used Bioconductor packages.\n"
     ]
    }
   ],
   "source": [
    "question = \"how many slots does the SingleCellExperiment object have?\"\n",
    "\n",
    "ask_question(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
